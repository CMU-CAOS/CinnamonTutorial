{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cinnamon: A Framework For Scale Out Encrypted AI\n",
    "## Notebook 3: Encrypted MNIST Inference, Parallelization Features of the Cinnamon Framework and the Cinnamon Accelerator Simulator\n",
    "\n",
    "In this tutorial, we will run an encrypted MNIST inference and use the Cinnamon compiler to parallelize the program. We will then use the Cinnamon accelerator simulator to evaulate Cinnamon's parallelization strategies.\n",
    "\n",
    "MNIST Model Credits: (Github: youben11)[https://github.com/youben11/encrypted-evaluation]\n",
    "\n",
    "Author:\n",
    "- Siddharth Jayashankar (sidjay@cmu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 \n",
    "In this exercise, we will write an encrypted CNN inference model for the MNIST dataset in the Cinnamon DSL. The MNIST dataset contains 28x28 images of handwritten digits 0 thorugh 9. The model classifies these digits into the corresponding category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MNIST CNN Model\n",
    "The input to the model is a 28x28 image from the MNIST dataset.\n",
    "\n",
    "Our model architecure is as follows:\n",
    "- A 2D convolution layer with 4 output channels, a kernel size of (7x7) and a stride of (3x3)\n",
    "- A square activation function\n",
    "- A 256x64 fully connected layer\n",
    "- A square activation function\n",
    "- A 64x10 fully connected layer\n",
    "\n",
    "The predicted digit is the argmax of the final layer.\n",
    "\n",
    "Notice the use a square activation function. This is because, the CKKS scheme can only express linear and polynomial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class MNIST_CNN(torch.nn.Module):\n",
    "    \"\"\"CNN for classifying MNIST data.\n",
    "    Input should be an encoded 28x28 matrix representing the image.\n",
    "    The input should also be normalized with a mean=0.1307 and an std=0.3081.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1,out_channels=4,kernel_size=(7,7),stride=(3,3),bias=True)\n",
    "        self.fc1 = torch.nn.Linear(in_features=256,out_features=64,bias=True)\n",
    "        self.fc2 = torch.nn.Linear(in_features=64,out_features=10,bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv2d(x)\n",
    "        conv_sq = conv * conv\n",
    "        conv_sq = conv_sq.reshape(1,-1)\n",
    "        o2 = self.fc1(conv_sq)\n",
    "        o2_sq = o2 * o2\n",
    "        o3 = self.fc2(o2_sq)\n",
    "        return o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained model and check it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# Load the image specified by sample_num and normalize it\n",
    "def load_input(sample_num):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    img = Image.open(f\"samples/img_{sample_num}.jpg\")\n",
    "    img = transform(img).view(1,28, 28).to(torch.float32)\n",
    "    return img\n",
    "\n",
    "# Get the labels of the images\n",
    "def get_labels():\n",
    "    with open('samples/answers.txt', 'r') as file:\n",
    "        labels = file.readlines()\n",
    "    labels = [int(l.rstrip('\\n')) for l in labels]\n",
    "    return labels\n",
    "\n",
    "# Compute the accuracy of the model over samples [1,num_samples] from the test set\n",
    "def accuracy(model, num_samples):\n",
    "    y = get_labels()\n",
    "    model = model.eval()\n",
    "    out = []\n",
    "    for i in range(1,num_samples+1):\n",
    "        input = load_input(i)\n",
    "        o = model(input)\n",
    "        o = torch.argmax(o)\n",
    "        out.append(o)\n",
    "    correct = torch.tensor([y[i] == out[i] for i in range(num_samples)],dtype=int)\n",
    "    return correct.float().mean()\n",
    "\n",
    "# Load the model\n",
    "model = MNIST_CNN()\n",
    "model.load_state_dict(torch.load(\"mnist.pth\"))\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# The test set has a total of 350 images. Let's check the accuracy of the model over all these images\n",
    "NUM_SAMPLES = 350\n",
    "model_accuracy = accuracy(model, 250)\n",
    "print(f\"Model accuracy on plain test_set: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model achieves a good accuracy of 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement an encrypted inference using the MNSIT model shown above in the Cinnamon DSL. We assume the model inputs (images) and activation values are encrypted while the model weights are plaintext values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cinnamon.dsl import *\n",
    "\n",
    "RNS_BIT_SIZE = 28\n",
    "TOP_LEVEL = 20\n",
    "\n",
    "## Helper function to create plaintext matrix inputs for the baby step giant step matrix multiplication \n",
    "def get_bsgs_plaintexts(name_base,babysteps,giantsteps,scale,level):\n",
    "    ret = []\n",
    "    for (g,gs) in enumerate(giantsteps):\n",
    "        for (b,bs) in enumerate(babysteps):\n",
    "            ret.append(PlaintextInput(f\"{name_base}_{bs}_{gs}\",scale,level))\n",
    "    return ret\n",
    "\n",
    "## Implement the Convolution operation. Refer to \n",
    "def conv_2d(image):\n",
    "    ## Implementing convolution for one output channel\n",
    "    def do_convolution(out_channel_id,image,result):\n",
    "        babysteps = [i * 8 for i in range(16)]\n",
    "        giantsteps = [i * 8192 for i in range(4)]\n",
    "        plaintexts = get_bsgs_plaintexts(f\"conv_weight_{out_channel_id}\",babysteps,giantsteps,scale=56,level=image.level())\n",
    "        product = bsgs(image,plaintexts,babysteps,giantsteps)\n",
    "        product = product.rescale()\n",
    "        bias = PlaintextInput(f\"conv_bias_{out_channel_id}\",product.scale(),product.level())\n",
    "        result[out_channel_id] = product + bias\n",
    " \n",
    "    output_channels = 4\n",
    "    outputs = [None for _ in range(output_channels)]\n",
    "    for o in range(output_channels):\n",
    "        do_convolution(o,image,outputs)\n",
    "\n",
    "    ## Stack all the output channels in a single ciphertext\n",
    "    for o in range(1,output_channels):\n",
    "        outputs[0] += outputs[o] >> (64*128*o)\n",
    "\n",
    "    return outputs[0].rescale().rescale()\n",
    "    \n",
    "\n",
    "def square(x):\n",
    "    return (x * x).relinearize()\n",
    "\n",
    "\n",
    "# Matrix multiplication of a 256x64 matrix with a 256x1 vector\n",
    "def matmul_256x64(v):\n",
    "    babysteps = [i * 128 for i in range(8)]\n",
    "    giantsteps = [i * 1024 for i in range(8)]\n",
    "    plaintexts = get_bsgs_plaintexts(f\"fc1_w\",babysteps,giantsteps,scale=56,level=v.level())\n",
    "    product = bsgs(v,plaintexts,babysteps,giantsteps)\n",
    "    product = product.rescale()\n",
    "    product += product << (1024*8)\n",
    "    product = product.rescale()\n",
    "    product += product >> (1024*16)\n",
    "    bias = PlaintextInput(f\"fc1_b\",product.scale(),product.level())\n",
    "    result = product + bias\n",
    "    return result\n",
    "\n",
    "# Matrix multiplication of a 64x10 matrix with a 64x1 vector\n",
    "def matmul_64x10(v):\n",
    "    babysteps = [i * 128 for i in range(4)]\n",
    "    giantsteps = [i * 512 for i in range(4)]\n",
    "    plaintexts = get_bsgs_plaintexts(f\"fc2_w\",babysteps,giantsteps,scale=56,level=v.level())\n",
    "    product = bsgs(v,plaintexts,babysteps,giantsteps)\n",
    "    product = product.rescale()\n",
    "    product += product << (1024*2)\n",
    "    product = product.rescale()\n",
    "    product += product << (1024*4)\n",
    "    bias = PlaintextInput(f\"fc2_b\",product.scale(),product.level())\n",
    "    result = product + bias\n",
    "    return result\n",
    "\n",
    "## Encrypted MNIST inference model\n",
    "def mnist(numChips=1):\n",
    "    mnistProgram = CinnamonProgram('Mnist',RNS_BIT_SIZE,num_chips=numChips)\n",
    "    with mnistProgram:\n",
    "        scale = 28*3\n",
    "        image = CiphertextInput('image',scale,TOP_LEVEL)\n",
    "        conv = conv_2d(image)\n",
    "        conv_sq = square(conv)\n",
    "        o2 = matmul_256x64(conv_sq.rescale())\n",
    "        o2_square = square(o2)\n",
    "        o3 = matmul_64x10(o2_square.rescale().rescale())\n",
    "        Output('pred',o3)\n",
    "\n",
    "    return mnistProgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I've implemented most of the model, one important component has been left as an exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1 Implement Plaintext Matrix Ciphertext Multiplication using Baby Step Giant Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baby Step Giant Step algorithm is a common algorithm to implement plaintext matrix times ciphertext matrix multiplication. The convolution and fully connected layers make use of this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impelement the BABY step giantestep algorithm\n",
    "def bsgs(input,M,babysteps,giantsteps):\n",
    "    ## TODO: Fill in the rotate_babysteps\n",
    "    rotate_babysteps = []\n",
    "    for (g,gs) in enumerate(giantsteps):\n",
    "        for (b,bs) in enumerate(babysteps):\n",
    "            i = g * len(babysteps) + b\n",
    "            if b == 0:\n",
    "                # TODO: Fill in the multiplication \n",
    "            else:\n",
    "                # TODO: Fill in the multiplication \n",
    "            else:\n",
    "        if g == 0:\n",
    "            # TODO: Fill in the giantsteps\n",
    "        else:\n",
    "            # TODO: Fill in the giantsteps\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's Compile The Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the compiler module\n",
    "from cinnamon.compiler import *\n",
    "from cinnamon.passes import *\n",
    "\n",
    "# Set The Number of chips to compile for\n",
    "numChips = 1\n",
    "# Set the directory where the Cinnamon compiler outputs should be created\n",
    "output_dir = \"outputs/\"\n",
    "!mkdir -p {output_dir}\n",
    "program = mnist(numChips)\n",
    "\n",
    "# Compile the program\n",
    "keyswitch_pass(program)\n",
    "cinnamon_compile(program, TOP_LEVEL, numChips, 256, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the inputs for our model. I've provided an internal method to take care of this in mnist_io.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_io import *\n",
    "\n",
    "# Returns the Program Inputs and Output Scales for the MNIST Program\n",
    "def mnist_io(sample_num):\n",
    "    input_image = load_input(sample_num)\n",
    "    input_image = input_image.detach().numpy()[0]\n",
    "    return get_mnist_program_io(input_image, TOP_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation. We will run only 1 sample here due to time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "def generate_secret_key(Slots,HammingWeight=32):\n",
    "    secretKey = [0]*(2*Slots)\n",
    "    count = 0\n",
    "    while count < HammingWeight:\n",
    "        pos = random.randint(0,2*Slots-1)\n",
    "        if secretKey[pos] != 0:\n",
    "            continue\n",
    "        val = random.randint(0,1)\n",
    "        if val == 0:\n",
    "            secretKey[pos] = -1\n",
    "        elif val == 1:\n",
    "            secretKey[pos] = 1\n",
    "        else:\n",
    "            raise Exception(\"\")\n",
    "        count += 1\n",
    "    return secretKey\n",
    "\n",
    "secretKey = generate_secret_key(SLOTS,HammingWeight=SLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cinnamon_emulator\n",
    "\n",
    "context = cinnamon_emulator.Context(SLOTS,Primes)\n",
    "\n",
    "encryptor = cinnamon_emulator.CKKSEncryptor(context,secretKey)\n",
    "emulator = cinnamon_emulator.Emulator(context)\n",
    "\n",
    "emulator.generate_and_serialize_evalkeys(f\"{output_dir}/evalkeys\",f\"{output_dir}/program_inputs\",encryptor)\n",
    "\n",
    "# This function runs a single encrypted inference over the image samples/image_{sample_id}.jpg \n",
    "def run_one_sample(sample_id):\n",
    "    print(f\"Running sample {sample_id}\")\n",
    "    Inputs, OutScale = mnist_io(sample_id)\n",
    "    emulator.generate_inputs(f\"{output_dir}/program_inputs\",f\"{output_dir}/evalkeys\",Inputs,encryptor)\n",
    "    emulator.run_program(f\"{output_dir}/instructions\",numChips,1024)\n",
    "    outputs = emulator.get_decrypted_outputs(encryptor,OutScale)\n",
    "    prediction = np.real(outputs[\"pred\"][0::128][0:10])\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "encrypted_predictions = []\n",
    "encrypted_predictions.append(run_one_sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have just experienced, running FHE on a CPU is very slow. And running all the samples in the dataset might take a while. Uncomment the code block below if you have the time to run more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_ENCRYPTED_SAMPLES = 20\n",
    "# for i in range(2,NUM_ENCRYPTED_SAMPLES+1):\n",
    "#     encrypted_predictions.append(run_one_sample(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the accuracy of our model on the few encrypted test samples we ran and compare it to the accuracy of the plaintext model on the set of test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(encrypted_predictions)\n",
    "y = get_labels()[:num_samples]\n",
    "correct = torch.tensor([int(y[i] == encrypted_predictions[i]) for i in range(num_samples)])\n",
    "encrypted_accuracy = correct.float().mean()\n",
    "\n",
    "plain_accuracy = accuracy(model,num_samples)\n",
    "print(f\"Encrypted Accuracy on {num_samples} samples: {encrypted_accuracy}\")\n",
    "print(f\"Plain Accuracy on {num_samples} samples:     {plain_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy of the encrypted model matches the accuracy of the plaintext model on the few samples we ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cinnamon Simulation Infrastructure\n",
    "Sadly, as you have just seen, FHE is very slow on CPUs. For FHE to be practical, hardware acceleration is essential. Cinnamon proposes a scale out hardware accelerator design for FHE. Let's see how we can simulate the program we just wrote on the Cinnamon accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation overview\n",
    "The Cinnamon simulator is a custom element built within the [SST](http://sst-simulator.org) simulation framework. This component is a cycle accurate simulator for the Cinnamon accelerator. The simulator evaluates the performance of programs compiled with the Cinnamon compiler on the Cinnamon accelerator. Let's simulate the encrypted MNIST inference we just compiled on the Cinnamon accelerator.\n",
    "\n",
    "To run the simulation in SST, we need to first create a set up an sst setup file. Take a look at the file [cinnamon-setup.py](cinnamon-setup.py). This file contains the default parameters for the Cinnamon architecture. To perform architectural studies, we can edit the cinnamon-setup.py file.\n",
    "\n",
    "Now, let's simulate the running of our encrypted MNIST inference programming on the Cinnamon accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the logs of the simulation\n",
    "sst_log_dir = \"sst_log_dir\"\n",
    "!mkdir -p {sst_log_dir}\n",
    "# Run the simulation on SST. Point the instructions_dir argument to the director where the compiler output was generated.\n",
    "!sst cinnamon-setup.py -- --instructions_dir=\"{output_dir}/\" > {sst_log_dir}/simulation_1.log\n",
    "# Print out the last 5 lines of the simulation log.\n",
    "!tail -n5 {sst_log_dir}/simulation_1.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quick wasn't it. Just took a few milli seconds! This shows the potential for hardware accelerators in making FHE practical. You can take a look at the [simulation log](sst_log_dir/simulation_1.log) to see the detailed output of the simulator. It contains detailed statistics on each component in the chip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1 Limb Level Parallelism using the Cinnamon Compiler\n",
    "\n",
    "CKKS ciphertexts in the RNS representation can be thought of as a matrix of modular integers. Each column is called a limb. In fact, the number of limbs in a ciphertext is its by it's level. The limbs of a ciphertext are largely data independent. There is good potential for parallelizing FHE computation by parallelizing the limbs. However, truly realizing this potential requires addressing the communication overheads introduced by cross limb dependencies. The figure below depicts limb level parallelism when the level is 4.\n",
    "\n",
    "![image](images/LimbLevelParallelism.jpg)\n",
    "\n",
    "Cinnamon developed novel algorithms and compiler techniques to realize the potential of limb level parallelism. The details are described in detail in the paper on the [Cinnamon framework](https://dl.acm.org/doi/pdf/10.1145/3669940.3707260). The Cinnamon compiler automatically implements limb level paralleism using Cinnamon's algorithms and techniques. To parallelize a program across Cinnamon chips, just change the `numChips` argument. That's it. It's that simple. The Cinnamon compiler will take care of the rest for you.\n",
    "\n",
    "\n",
    "Let's compile the MNIST inference program for Cinnamon-4 by setting the `numChips` argument to 4. The Cinnamon compiler will generate a sequence of instructions for each of the 4 chips and insert the appropriate synchronization and communication instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numChips = 4\n",
    "output_dir = f\"outputs_{numChips}ch/\"\n",
    "!mkdir -p {output_dir}\n",
    "program = mnist(numChips)\n",
    "keyswitch_pass(program)\n",
    "cinnamon_compile(program, TOP_LEVEL, numChips, 256, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Cinnamon emulator to run the program we just compiled. The Cinnamon emulator runs each chip's instructions as a multi threaded program with `numChips` threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cinnamon_emulator\n",
    "context = cinnamon_emulator.Context(SLOTS,Primes)\n",
    "\n",
    "encryptor = cinnamon_emulator.CKKSEncryptor(context,secretKey)\n",
    "emulator = cinnamon_emulator.Emulator(context)\n",
    "\n",
    "\n",
    "\n",
    "emulator.generate_and_serialize_evalkeys(f\"{output_dir}/evalkeys\",f\"{output_dir}/program_inputs\",encryptor)\n",
    "encrypted_predictions = []\n",
    "encrypted_predictions.append(run_one_sample(1))\n",
    "print(f\"Encrypted Limb Level Parallel Predictions: {encrypted_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the emulated progran too ran faster. Now, let's use the Cinnamon simulator to see how limb level parallelism using 4 chips speeds up our program. We pass the `--chips` command line argument to the setup file to specify that this program is to be run on 4 chips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sst cinnamon-setup.py -- --instructions_dir=\"{output_dir}/\" --chips={numChips} > {sst_log_dir}/simulation_{numChips}.log\n",
    "!tail -n5 {sst_log_dir}/simulation_{numChips}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was much faster than the 1 chip example. You can take a look at the [simulation logfile](sst_log_dir/simulation_4.log) to see a detailed report of the simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2 Program Level Parallelism using the Cinnamon Compiler.\n",
    "\n",
    "The example above distributes the limbs of all ciphertexts modulo the 4 chips. However, there is another dimension along which our program can be parallelized - program level parallelism. Program level parallelism parallelizes computation at the ciphertext level. In our encrypted MNIST program, we can exploit this kind of parallelism in the convolution layer. Looking into the convolution operation, we see that we perform the operation 4 times, to get a four channel output. Now, instead of parallelizing each of the four iterations at the limb level, we could parallelize the four iterations of the convolution across the 4 chips. This kind of parallelism is called program parallelism.\n",
    "\n",
    "In the Cinnamon DSL, program parallelism is implemented using `CinnamonStreams`. Each stream can be thought of as a concurrent thread that executes on the number of chips specified by the `StreamSize` argument. The number of streams to be created is specified by the `NumStreams` argument, with each stream receiving a streamID in 0 through `NumStreams-1`. Within a single stream, the values are parallelized at the limb level. The `streamFn` argument specifies the function that each stream implements. The first argument passed to the stream function is the streamId. The rest of the arguments are function specific.  \n",
    "\n",
    "In this example, we set `StreamSize=1` and `NumStreams=4`. This creates four concurrent streams with each stream running on 1 chip and implementing the `do_convolution` function. The figure below illustrates  program paralleism and limb parallelism.\n",
    "\n",
    "![image](images/LimbAndProgramParalleism.jpg)\n",
    "\n",
    "By default, the program runs in a single stream with `StreamSize=numChips`. Thus, the rest of the program will remain limb level parallelized across the four chips, just as in the previous example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Program Level Parallelism implementation of convolution\n",
    "def conv_2d(image):\n",
    "    def do_convolution(out_channel_id,image,result):\n",
    "        babysteps = [i * 8 for i in range(16)]\n",
    "        giantsteps = [i * 8192 for i in range(4)]\n",
    "        plaintexts = get_bsgs_plaintexts(f\"conv_weight_{out_channel_id}\",babysteps,giantsteps,scale=56,level=image.level())\n",
    "        product = bsgs(image,plaintexts,babysteps,giantsteps)\n",
    "        product = product.rescale()\n",
    "        bias = PlaintextInput(f\"conv_bias_{out_channel_id}\",product.scale(),product.level())\n",
    "        result[out_channel_id] = product + bias\n",
    " \n",
    "    output_channels = 4\n",
    "    outputs = [None for _ in range(output_channels)]\n",
    "\n",
    "    # for o in range(output_channels):\n",
    "    #     do_convolution(o,image,outputs)\n",
    "\n",
    "    print(\"Compiling conv2D using Cinnamon Streams\")\n",
    "    # TODO: Use program level parallelism to parallelize the four iterations of the convolution.\n",
    "    CinnamonStream(StreamSize=1,NumStreams=4,StreamFn=do_convolution,image=image,result=outputs)\n",
    "\n",
    "\n",
    "    for o in range(1,output_channels):\n",
    "        outputs[0] += outputs[o] >> (64*128*o)\n",
    "\n",
    "    return outputs[0].rescale().rescale()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile this program, emulate an inference and evaluate it's performance on the Cinnamon accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compile for a 4 chip Cinnamon accelerator, set numChips to 4 \n",
    "numChips = 4\n",
    "output_dir = f\"outputs_{numChips}ch_programParallel/\"\n",
    "!mkdir -p {output_dir}\n",
    "program = mnist(numChips)\n",
    "keyswitch_pass(program)\n",
    "cinnamon_compile(program, TOP_LEVEL, numChips, 256, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulator.generate_and_serialize_evalkeys(f\"{output_dir}/evalkeys\",f\"{output_dir}/program_inputs\",encryptor)\n",
    "encrypted_predictions = []\n",
    "encrypted_predictions.append(run_one_sample(1))\n",
    "print(f\"Encrypted Program Parallel Predictions: {encrypted_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sst cinnamon-setup.py -- --instructions_dir=\"outputs_{numChips}ch_programParallel/\" --chips={numChips} > {sst_log_dir}/simulation_{numChips}_program_parallel.log\n",
    "!tail -n5 {sst_log_dir}/simulation_{numChips}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cinnamon Keyswitch Pass and Cinnamon's New Parallel Keyswitching Algorithms\n",
    "\n",
    "Cinnamon introduces two new parallel keyswitching algorithms: Input Broadcast Keyswitching and Output Aggregation Keyswitching. These two parallel keyswitching algorithms are designed with the objective of minimizing the inter chip communication. These algorithms work by looking for specific program patterns and reordering operations and trading off communication for compute. The `keyswitch_pass` in the cinnamon compiler implements looks for program patterns that the  . The two new parallel keyswitching algorithms and the `keyswitch` pass are intendend to work together to minimize communcation overheads and deliver performance gains. Let's run an experiment using the Cinnamon framework to evalutate these techniques.\n",
    "\n",
    "In this experiment, we will compile three different versions of the mnist program:\n",
    "- **unoptimized**: With keyswitch_pass and cinnamon's parallel keyswitching algorithms disabled.\n",
    "- **pass**: With just keyswitch_pass enabled and cinnamon's parallel keyswitching algorithms disabled.\n",
    "- **pass_cinnamon_ks**: With both keyswitch_pass and cinnamon's new parallel keyswitching algorithms enabled.\n",
    "\n",
    "We will evaluate these 3 configurations using 4 Cinnamon chips. The default value of the inter chip link bandwidth in Cinnamon is 256GB/s. However in this experiment, we will compare how these three configurations perform across a range of interchip bandwidths from 128GB/s to 1TB/s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reset the convolution back to the version without program parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Convolution to use Limb Level Parallelism\n",
    "def conv_2d(image):\n",
    "    def do_convolution(out_channel_id,image,result):\n",
    "        babysteps = [i * 8 for i in range(16)]\n",
    "        giantsteps = [i * 8192 for i in range(4)]\n",
    "        plaintexts = get_bsgs_plaintexts(f\"conv_weight_{out_channel_id}\",babysteps,giantsteps,scale=56,level=image.level())\n",
    "        product = bsgs(image,plaintexts,babysteps,giantsteps)\n",
    "        product = product.rescale()\n",
    "        bias = PlaintextInput(f\"conv_bias_{out_channel_id}\",product.scale(),product.level())\n",
    "        result[out_channel_id] = product + bias\n",
    " \n",
    "    output_channels = 4\n",
    "    outputs = [None for _ in range(output_channels)]\n",
    "\n",
    "    print(\"Compiling conv2D using Limb Level Parallelism\")\n",
    "    for o in range(output_channels):\n",
    "        do_convolution(o,image,outputs)\n",
    "\n",
    "    for o in range(1,output_channels):\n",
    "        outputs[0] += outputs[o] >> (64*128*o)\n",
    "\n",
    "    return outputs[0].rescale().rescale()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compile the three program in the three configurations we listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numChips = 4\n",
    "program = mnist(numChips)\n",
    "\n",
    "## unoptimized\n",
    "output_dir = f\"outputs_{numChips}ch_unoptimized/\"\n",
    "!mkdir -p {output_dir}\n",
    "# Keyswitch Pass is not enabled\n",
    "cinnamon_compile(program, TOP_LEVEL, numChips, 256, output_dir, use_cinnamon_keyswitching=False)\n",
    "\n",
    "## pass\n",
    "output_dir = f\"outputs_{numChips}ch_pass/\"\n",
    "!mkdir -p {output_dir}\n",
    "keyswitch_pass(program)\n",
    "cinnamon_compile(program, TOP_LEVEL, numChips, 256, output_dir, use_cinnamon_keyswitching=False)\n",
    "\n",
    "## pass_cinnamon_ks\n",
    "output_dir = f\"outputs_{numChips}ch_pass_cinnamon_ks/\"\n",
    "!mkdir -p {output_dir}\n",
    "keyswitch_pass(program)\n",
    "cinnamon_compile(program, TOP_LEVEL, numChips, 256, output_dir, use_cinnamon_keyswitching=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `--linkBW` argument in [cinnamon-setup.py](cinnamon-setup.py) to specify the inter chip link bandwidth to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_dir = \"bandwidth_expt_dir\"\n",
    "configs = ['unoptimized','pass','pass_cinnamon_ks']\n",
    "\n",
    "bandwidths = [\"0.125\",\"0.25\",\"0.5\",\"1\"] # in TB/s\n",
    "for c in configs:\n",
    "    !mkdir -p {expt_dir}/{c}\n",
    "for bw in bandwidths:\n",
    "    for c in configs:\n",
    "        !sst cinnamon-setup.py -- --instructions_dir=\"outputs_{numChips}ch_{c}/\" --chips={numChips} --linkBW={bw} > {expt_dir}/{c}/{bw}bw.log\n",
    "print(\"Experiments Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[rollup.py](rollup.py) implements utilities for reading the logfiles and plotting the relative performance of the three configs across the bandwidths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rollup\n",
    "rollup.plot_speedups(expt_dir,configs,bandwidths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots depict how Cinnamon's new keyswitching alogorithms and compiler passes can deliver speedup for scale out encrypted AI applications. This marks the end of the tutorial. Thank you taking part!."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
